#!/usr/bin/env python3
#!/opt/clmgr/python-310/bin/python3.10

import sys
import math
import random
import time
from timeit import default_timer as timer
import io

import threading
from concurrent.futures.thread import ThreadPoolExecutor
from concurrent.futures import wait

from ClusterShell.NodeSet import NodeSet, expand
from ClusterShell.Task import Task, task_self

import cli

# ansible_playbook = "/opt/ncar/ansible/bin/ansible-playbook"
# inventory = "derecho_inventory.yml"
# playbook = "nwsc3-playbook.yml"
# vault_password_file = "/root/ansible.passwd"

# Parse cli optsions
args = cli.parser.parse_args()
# print(args) # Remove me when finished adding all the extra args.

ansible_playbook_cmd = args.ansible_playbook_cmd
ansible_dir = args.ansible_dir
ansible_playbook = args.playbook
ansible_inventory = args.inventory_file
ansible_playbook = args.playbook
vault_password_file = args.vault_password_file
base_log_dir = args.base_log_dir

target_nodeset = args.limit
runner_nodeset = args.runners
nforks = args.forks

if args.check:
    check_mode = "--check"
else:
    check_mode = ""

print("Clushible starting.")

if args.verbose > 0:
    cli.utils.info(f"Ansible playbook cmd: {ansible_playbook_cmd}")
    cli.utils.info(f"Ansible directory:    {ansible_dir}")
    cli.utils.info(f"Ansible playbook:     {ansible_playbook}")
    cli.utils.info(f"Ansible inventory:    {ansible_inventory}")
    cli.utils.info(f"Vault password File:  {vault_password_file}")
    cli.utils.info(f"Target nodes:         {target_nodeset}")
    cli.utils.info(f"Runner nodes:         {runner_nodeset}")
    cli.utils.info(f"Fork count:           {nforks}")
    cli.utils.info(f"Base Log Directory:   {base_log_dir}")

if args.dry_run:
    print("\n ** DRY RUN ** \n")

if len(target_nodeset) < nforks:
    cli.warn("nodeset count less than forks, setting forks to nodeset size.")
    nforks = len(target_nodeset)
    nsets = 1
else:
    nsets = math.ceil(len(target_nodeset) / nforks)

if args.verbose:
    print(f"Number of sets:  {nsets}")

subsets = []
for each in target_nodeset.split(nsets):
    subsets.append(each)

# Get Runners
alive_cmd = "/usr/bin/nproc"
if args.dry_run:
    for n in runner_nodeset:
        print(f"{n}: {alive_cmd}")
    print("")
else:
    R = task_self()
    R.run(alive_cmd, nodes=runner_nodeset)

    for rc, nodelist in R.iter_retcodes():
        n = NodeSet.fromlist(nodelist)
        print(f"{n} (RC={rc})")

        # if anything does not return 0a
        if rc != 0:
            runner_nodeset.remove(n)

    if len(runner_nodeset) == 0:
        print("Fatal Error: no runners suitable. Exiting.")
        sys.exit(1)

    for output, nodelist in R.iter_buffers():
        print(f"{NodeSet.fromlist(nodelist)}: {output.message()}")

cli.utils.info(f"Suitable runners: {runner_nodeset}\n")

# Determine whether or not we should pack and/or spread the load. May need to
# modify up a bit with the nsets

# Send the vault password file to runners

# Send the ansible-directory to the runners.
# This shouldn't be HUGE, but NFS is possible too which is what was used in PoC

# Generate the commands needing to be run against specific nodes from arbitrary runner
cmds = []
idx = 0
for s in subsets:
    idx = idx + 1
    # Create Target Range for Ansible
    i = ",".join(expand(s))
    if args.verbose > 0:
        cli.utils.info(f"Nodes count in subset {idx}: {len(s)}")

    # Generate Commands
    # playbook_cmd = f"cd /opt/ncar/hsg-ansible; {ansible_playbook} -i {inventory} -f {nforks} --limit {i} --vault-password-file {vault_password_file} {playbook} &>> /tmp/ansible.clushible.cpu"
    playbook_cmd = " ".join(
        f"""
    mkdir -p {base_log_dir};
    LOGDIR=$(mktemp -d {base_log_dir});
    cd {ansible_dir};
    {ansible_playbook_cmd} -i {ansible_inventory} \
        -f {nforks} \
        --limit {i} \
        --vault-password-file {vault_password_file} \
        {ansible_playbook} &>> \$LOGDIR/ansible.clushible.log";
    """.strip()
        .replace("\n", " ")
        .split()
    )
    # Add to list
    cmds.append(playbook_cmd)

print("\nCommands to be executed:")
idx = 0
for cmd in cmds:
    idx = idx + 1
    print(f"\t{idx}: Command: {cmd}\n")

# Create a series of tasks list
T = task_self()


# wrapper around tasks to run under threads
def run_task(cmd):
    # Get threadid
    th = threading.current_thread()
    # t = task_self()
    t = Task(th)
    n = thread_to_runner[th.ident]
    if args.verbose > 0:
        cli.utils.info(f"Runner {n}: {cmd}")

    # Timed tasks.
    start = timer()
    t.run(cmd, nodes=n)
    t.join()
    end = timer()
    dt = end - start
    print(f"delta_time: {dt}")

    return t.max_retcode()


def check_th(silly):
    th = threading.current_thread()
    # print(threading.get_ident())
    start = timer()
    time.sleep(1)
    end = timer()
    dt = end - start
    print(f"Thread Check Time: {timer() - start:.4f}")


grange = [i for i in range(len(runner_nodeset))]
thread_to_runner = dict()

# Too lame to create my own thread pool
with ThreadPoolExecutor(max_workers=len(runner_nodeset)) as executor:
    main_th = threading.current_thread()
    start = timer()
    reg_threads = [executor.submit(check_th, i) for i in grange]
    wait(reg_threads)
    print(f"\nExecutor registration time: {timer() - start:0.4f}")

    r_list = list(runner_nodeset)
    for i in threading.enumerate():
        if main_th == i:
            continue
        thread_to_runner[i.ident] = r_list.pop(0)

    if args.verbose > 2:
        for k, v in thread_to_runner:
            cli.utils.info(f"Runner ({k}) has registered thread {k}.")

    for c in cmds:
        if not args.dry_run:
            executor.submit(run_task, c)
        else:
            print(f"\nWould run the following command in executor pool:")
            print(f"\n{c}\n")

sys.exit(0)

for c in cmds:
    T.run(c, nodes=random.choice(list(runners)))
    for buf, nodelist in T.iter_buffers():
        print(buf.message())

    print(T.max_retcode())

print("Done")
